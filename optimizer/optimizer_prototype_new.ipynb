{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import single_node_profiles_cpp as snp\n",
    "import profiler_new\n",
    "import numpy as np\n",
    "from optimizer_new import BruteForceOptimizer, GreedyOptimizer\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profs = snp.load_single_node_profiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'tf-resnet-feats',\n",
       " u'tf-log-reg',\n",
       " u'tf-kernel-svm',\n",
       " u'res50',\n",
       " u'inception',\n",
       " u'res152',\n",
       " u'alexnet']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inception\n"
     ]
    }
   ],
   "source": [
    "dag = profiler_new.get_logical_pipeline(\"pipeline_one\")\n",
    "with open(os.path.abspath(\"../results_python_benchmarker/e2e_profs_new_metrics/incep_1-logreg_1-ksvm_1-resnet_1-180207_063416.json\")) as f:\n",
    "    sample_run = json.load(f)\n",
    "print(dag.reference_node)\n",
    "scale_factors = profiler_new.get_node_scale_factors(sample_run, dag.reference_node)\n",
    "node_configs = profiler_new.get_node_configs_from_experiment(sample_run)\n",
    "def which_stage(model_name):\n",
    "    if model_name == \"tf-kernel-svm\" or model_name == \"tf-log-reg\":\n",
    "        return \"latency_stage\"\n",
    "    else:\n",
    "        return \"thru_stage\"\n",
    "node_profs = {name : profiler_new.NodeProfile(name, profs[name], which_stage(name)) for name, _ in node_configs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<profiler_new.NodeProfile at 0x154f15d2d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inception = profiler_new.NodeProfile(\"inception\", profs[\"inception\"], \"thru_stage\")\n",
    "inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service throughput lower than arrival rate!\n",
      "('tf-resnet-feats', 21.364568571428574, inf, {'inception': NodeConfig(inception, 1, v100, 1, 1, aws), 'tf-kernel-svm': NodeConfig(tf-kernel-svm, 1, none, 1, 1, aws), 'tf-resnet-feats': NodeConfig(tf-resnet-feats, 1, v100, 1, 2, aws), 'tf-log-reg': NodeConfig(tf-log-reg, 1, none, 1, 1, aws)})\n",
      "Increasing replication_factor not taken. Latency: 0.11687844 (0.5) Cost: 7.9165 (5.5)\n",
      "Service throughput lower than arrival rate!\n",
      "('tf-resnet-feats', 21.170955714285714, inf, {'inception': NodeConfig(inception, 1, v100, 1, 1, aws), 'tf-kernel-svm': NodeConfig(tf-kernel-svm, 1, none, 1, 1, aws), 'tf-resnet-feats': NodeConfig(tf-resnet-feats, 1, v100, 2.0, 1, aws), 'tf-log-reg': NodeConfig(tf-log-reg, 1, none, 1, 1, aws)})\n",
      "Upgrading bottleneck node tf-resnet-feats to NodeConfig(tf-resnet-feats, 1, v100, 2.0, 1, aws)\n",
      "('batch_size', 0.11817702000000001, inf, inf)\n",
      "Service throughput lower than arrival rate!\n",
      "('inception', 29.128397142857143, inf, {'inception': NodeConfig(inception, 1, v100, 1, 1, aws), 'tf-kernel-svm': NodeConfig(tf-kernel-svm, 1, none, 1, 1, aws), 'tf-resnet-feats': NodeConfig(tf-resnet-feats, 1, v100, 2.0, 2, aws), 'tf-log-reg': NodeConfig(tf-log-reg, 1, none, 1, 1, aws)})\n",
      "Increasing replication_factor not taken. Latency: 0.11817702 (0.5) Cost: 7.9165 (5.5)\n",
      "Service throughput lower than arrival rate!\n",
      "('inception', 29.128397142857143, inf, {'inception': NodeConfig(inception, 1, v100, 1, 1, aws), 'tf-kernel-svm': NodeConfig(tf-kernel-svm, 1, none, 1, 1, aws), 'tf-resnet-feats': NodeConfig(tf-resnet-feats, 1, v100, 3.0, 1, aws), 'tf-log-reg': NodeConfig(tf-log-reg, 1, none, 1, 1, aws)})\n",
      "Upgrading bottleneck node tf-resnet-feats to NodeConfig(tf-resnet-feats, 1, v100, 3.0, 1, aws)\n",
      "('batch_size', 0.11994274, inf, inf)\n",
      "Service throughput lower than arrival rate!\n",
      "('tf-resnet-feats', 31.392415714285715, inf, {'inception': NodeConfig(inception, 1, v100, 1, 2, aws), 'tf-kernel-svm': NodeConfig(tf-kernel-svm, 1, none, 1, 1, aws), 'tf-resnet-feats': NodeConfig(tf-resnet-feats, 1, v100, 3.0, 1, aws), 'tf-log-reg': NodeConfig(tf-log-reg, 1, none, 1, 1, aws)})\n",
      "Increasing replication_factor not taken. Latency: 0.11994274 (0.5) Cost: 7.9165 (5.5)\n",
      "Service throughput lower than arrival rate!\n",
      "('tf-resnet-feats', 31.392415714285715, inf, {'inception': NodeConfig(inception, 1, v100, 2.0, 1, aws), 'tf-kernel-svm': NodeConfig(tf-kernel-svm, 1, none, 1, 1, aws), 'tf-resnet-feats': NodeConfig(tf-resnet-feats, 1, v100, 3.0, 1, aws), 'tf-log-reg': NodeConfig(tf-log-reg, 1, none, 1, 1, aws)})\n",
      "Upgrading bottleneck node inception to NodeConfig(inception, 1, v100, 2.0, 1, aws)\n",
      "('batch_size', 0.11994274, inf, inf)\n",
      "Initializing maximum x coordinate\n",
      "Telescoping on x coordinate\n",
      "(2048.0, 4096.0, 3072.0, 165.62513334857144, 168)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6d3e8edbb110>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# 24 ms mean inter-arrival time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0marrival_cached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexponential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_optimal_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrival_cached\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize_what\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"throughput\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/cusgadmin/Desktop/plots-model-comp-paper/optimizer/optimizer_new.py\u001b[0m in \u001b[0;36mselect_optimal_config\u001b[0;34m(self, cloud, latency_constraint, cost_constraint, initial_config, arrival_history, optimize_what)\u001b[0m\n\u001b[1;32m    257\u001b[0m                     \u001b[0;31m# (the maximum queue size) isn't correct anymore, and that the second result (response time) no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                     \u001b[0;31m# longer includes the service time, so just the queue waitng time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_waiting_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrival_history_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_max_Q_and_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_estimated_perf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"throughput\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Convert throughput to queries per ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m                     \u001b[0mresponse_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_estimated_perf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"latency\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mQ_waiting_time\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000.\u001b[0m \u001b[0;31m# converting time to seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew_bottleneck_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_estimated_perf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"throughput\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopied_pipeline_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cusgadmin/Desktop/plots-model-comp-paper/optimizer/optimizer_new.py\u001b[0m in \u001b[0;36mget_max_Q_and_time\u001b[0;34m(self, latency, throughput)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# accurate estimate plot of the arrival curve, but also requires more computation time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0marrival_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0marrival_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_arrival_curve_at_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrival_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         return (self._max_Q_given_arrival(arrival_x, arrival_y, latency, throughput),\n\u001b[1;32m    189\u001b[0m                 self._max_response_time_given_arrival(arrival_x, arrival_y, latency, throughput))\n",
      "\u001b[0;32m/Users/cusgadmin/Desktop/plots-model-comp-paper/optimizer/optimizer_new.py\u001b[0m in \u001b[0;36m_get_arrival_curve_at_x\u001b[0;34m(self, arrival_x_value)\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0mmax_so_far\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontained_currently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmax_so_far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_smallest_delta_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrival_x_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;31m# print(\"_get_arrival_curve_at_x(\"+str(arrival_x_value)+\") = \"+str(result))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cusgadmin/Desktop/plots-model-comp-paper/optimizer/optimizer_new.py\u001b[0m in \u001b[0;36mget_smallest_delta_2\u001b[0;34m(time_range, timestamps)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mmax_so_far\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontained_currently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mhead_index\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# this means time_range's higher end hasn't exceeded the very last timestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tail\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                     \u001b[0mhead_time_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimestamps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtail_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtime_range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0mhead_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimestamps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhead_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhead_time_position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from optimizer_new import BruteForceOptimizer, GreedyOptimizer\n",
    "opt = GreedyOptimizer(dag, scale_factors, node_profs)\n",
    "cloud = \"aws\"\n",
    "initial_config = {\"tf-resnet-feats\": profiler_new.NodeConfig(name=\"tf-resnet-feats\",\n",
    "                                                          num_cpus=1,\n",
    "                                                          gpu_type=\"v100\",\n",
    "                                                          batch_size=1,\n",
    "                                                          num_replicas=1,\n",
    "                                                          cloud=cloud),\n",
    "                  \"inception\": profiler_new.NodeConfig(name=\"inception\",\n",
    "                                                      num_cpus=1,\n",
    "                                                      gpu_type=\"v100\",\n",
    "                                                      batch_size=1,\n",
    "                                                      num_replicas=1,\n",
    "                                                      cloud=cloud),\n",
    "                  \"tf-log-reg\": profiler_new.NodeConfig(name=\"tf-log-reg\",\n",
    "                                                      num_cpus=1,\n",
    "                                                      gpu_type=\"none\",\n",
    "                                                      batch_size=1,\n",
    "                                                      num_replicas=1,\n",
    "                                                      cloud=cloud),\n",
    "                  \"tf-kernel-svm\": profiler_new.NodeConfig(name=\"tf-kernel-svm\",\n",
    "                                                      num_cpus=1,\n",
    "                                                      gpu_type=\"none\",\n",
    "                                                      batch_size=1,\n",
    "                                                      num_replicas=1,\n",
    "                                                      cloud=cloud),\n",
    "                 }\n",
    "# 24 ms mean inter-arrival time\n",
    "arrival_cached = np.cumsum(np.random.exponential(24, size=(40000)))\n",
    "opt.select_optimal_config(cloud, 0.5, 5.5, initial_config, arrival_cached, optimize_what=\"throughput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"arrival_deltas_ms\", 'w') as f:\n",
    "    for delta in np.diff(arrival_cached):\n",
    "        f.write(\"{}\\n\".format(delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def brute_force_optimizer(dag, scale_factors, node_profs, cost_constraint, latency_constraint):\n",
    "    \"\"\"\n",
    "        This doesn't loo\n",
    "    \"\"\"\n",
    "    all_node_configs = [node_profs[node].enumerate_configs(max_replication_factor=3) for node in dag.nodes()]     \n",
    "    all_pipeline_configs = itertools.product(*all_node_configs)\n",
    "    num_valid_configs = 0\n",
    "    best_config = None\n",
    "    best_config_perf = None\n",
    "    cur_index = 0\n",
    "    for p_config in all_pipeline_configs:\n",
    "        cur_index += 1\n",
    "        if cur_index % 500 == 0:\n",
    "            print(\"Processed {}\".format(cur_index))\n",
    "        cur_node_configs = {n.name: n for n in p_config}\n",
    "        if not profiler.is_valid_pipeline_config(cur_node_configs):\n",
    "            continue\n",
    "        cur_config_perf = profiler.estimate_pipeline_performance_for_config(\n",
    "            dag, scale_factors, cur_node_configs, node_profs)\n",
    "        if cur_config_perf[\"latency\"] <= latency_constraint and cur_config_perf[\"cost\"] <= cost_constraint:\n",
    "            if best_config is None:\n",
    "                best_config = cur_node_configs\n",
    "                best_config_perf = cur_config_perf\n",
    "                print(\"Initializing config to {} ({})\".format(best_config, best_config_perf))\n",
    "            else:\n",
    "                if cur_config_perf[\"throughput\"] > best_config_perf[\"throughput\"]:\n",
    "                    best_config = cur_node_configs\n",
    "                    best_config_perf = cur_config_perf\n",
    "                    print(\"Updating config to {} ({})\".format(best_config, best_config_perf))\n",
    "        \n",
    "    return best_config, best_config_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "brute_force_optimizer(dag, scale_factors, node_profs, 7.0, 0.8)\n",
    "end = datetime.now()\n",
    "print(\"{}\".format((end-start).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profiler.estimate_pipeline_performance_for_config(dag, scale_factors, node_configs, node_profs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = profs[\"alexnet\"].groupby([\"cloud\",\"gpu_type\",\"num_cpus_per_replica\"])\n",
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, df in groups:\n",
    "    print(name, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
