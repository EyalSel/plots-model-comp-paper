{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import single_node_profiles_cpp as snp\n",
    "import profiler\n",
    "import end_to_end_profiles as e2e_profs\n",
    "import numpy as np\n",
    "from optimizer import BruteForceOptimizer, GreedyOptimizer\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# Create logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create STDERR handler\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "# ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatter and add it to the handler\n",
    "formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# Set STDERR handler as the only handler \n",
    "logger.handlers = [handler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profs = snp.load_single_node_profiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dag = profiler.get_logical_pipeline(\"pipeline_one\")\n",
    "with open(os.path.abspath(\"../results_python_benchmarker/e2e_profs/systemx/image_driver_1/500ms/incep_1-logreg_1-ksvm_1-resnet_1-171221_091209.json\")) as f:\n",
    "    sample_run = json.load(f)\n",
    "scale_factors = profiler.get_node_scale_factors(sample_run, dag.reference_node)\n",
    "node_configs = profiler.get_node_configs_from_experiment(sample_run)\n",
    "node_profs = {}\n",
    "for name, _ in node_configs.items():\n",
    "    if name in [\"tf-log-reg\", \"tf-kernel-svm\"]:\n",
    "        node_profs[name] = profiler.NodeProfile(name, profs[name], \"latency_stage\")\n",
    "    else:\n",
    "        node_profs[name] = profiler.NodeProfile(name, profs[name], \"thru_stage\")\n",
    "\n",
    "\n",
    "# node_profs = {name : profiler.NodeProfile(name, profs[name]) for name, _ in node_configs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000186616787687\n",
      "0.047\n"
     ]
    }
   ],
   "source": [
    "with open(\"../debugging/arrival_deltas_ms.timestamp\", \"r\") as f:\n",
    "    arrival_deltas = np.array([float(l.strip()) for l in f]).flatten()\n",
    "    print(np.min(arrival_deltas))\n",
    "    arrival_deltas = np.clip(arrival_deltas, a_min=0.047, a_max=None)\n",
    "    print(np.min(arrival_deltas))\n",
    "\n",
    "    arrival_history = np.cumsum(arrival_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results_cpp_benchmarker/e2e_results/image_driver_1/cpp-aws-p2-1-inception-1-resnet-3-ksvm-1-logreg-180324_170634.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "    resnet_lineage = results[\"throughput_results\"][\"lineage\"][\"tf-resnet-feats\"][:40000]\n",
    "    send_times = [l[\"driver::send\"] for l in resnet_lineage]\n",
    "    send_times = np.diff(send_times) / 1000.0\n",
    "\n",
    "with open(\"../debugging/measured_arrival_deltas_ms.timestamp\", \"w\") as f:\n",
    "    for s in send_times:\n",
    "        f.write(\"{}\\n\".format(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18-03-28:11:18:37 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:18:59 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:19:23 INFO     [optimizer.py:337] Upgrading bottleneck node tf-resnet-feats to NodeConfig(tf-resnet-feats, 1, v100, 2.0, 1, aws)\n",
      "18-03-28:11:19:23 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:19:53 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:20:17 INFO     [optimizer.py:337] Upgrading bottleneck node tf-resnet-feats to NodeConfig(tf-resnet-feats, 1, v100, 3.0, 1, aws)\n",
      "18-03-28:11:20:17 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:20:44 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:21:13 INFO     [optimizer.py:337] Upgrading bottleneck node inception to NodeConfig(inception, 1, v100, 2.0, 1, aws)\n",
      "18-03-28:11:21:13 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:21:36 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:21:59 INFO     [optimizer.py:337] Upgrading bottleneck node tf-resnet-feats to NodeConfig(tf-resnet-feats, 1, v100, 4.0, 1, aws)\n",
      "18-03-28:11:21:59 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:22:27 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:22:50 INFO     [optimizer.py:337] Upgrading bottleneck node tf-resnet-feats to NodeConfig(tf-resnet-feats, 1, v100, 8.0, 1, aws)\n",
      "18-03-28:11:22:50 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:23:12 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:23:33 INFO     [optimizer.py:337] Upgrading bottleneck node inception to NodeConfig(inception, 1, v100, 3.0, 1, aws)\n",
      "18-03-28:11:23:33 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:23:54 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:24:15 INFO     [optimizer.py:337] Upgrading bottleneck node inception to NodeConfig(inception, 1, v100, 4.0, 1, aws)\n",
      "18-03-28:11:24:15 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:24:37 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:24:58 INFO     [optimizer.py:337] Upgrading bottleneck node tf-resnet-feats to NodeConfig(tf-resnet-feats, 1, v100, 12.0, 1, aws)\n",
      "18-03-28:11:24:58 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:25:19 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:25:41 INFO     [optimizer.py:337] Upgrading bottleneck node tf-kernel-svm to NodeConfig(tf-kernel-svm, 1, none, 2.0, 1, aws)\n",
      "18-03-28:11:25:41 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:26:04 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:26:35 INFO     [optimizer.py:337] Upgrading bottleneck node inception to NodeConfig(inception, 1, v100, 8.0, 1, aws)\n",
      "18-03-28:11:26:35 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:27:01 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:27:26 INFO     [optimizer.py:337] Upgrading bottleneck node tf-resnet-feats to NodeConfig(tf-resnet-feats, 1, v100, 16.0, 1, aws)\n",
      "18-03-28:11:27:26 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:27:52 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:28:16 INFO     [optimizer.py:337] Upgrading bottleneck node tf-kernel-svm to NodeConfig(tf-kernel-svm, 1, none, 3.0, 1, aws)\n",
      "18-03-28:11:28:16 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:28:38 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:29:00 INFO     [optimizer.py:337] Upgrading bottleneck node tf-resnet-feats to NodeConfig(tf-resnet-feats, 1, v100, 24.0, 1, aws)\n",
      "18-03-28:11:29:00 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:29:23 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:29:44 INFO     [optimizer.py:337] Upgrading bottleneck node inception to NodeConfig(inception, 1, v100, 12.0, 1, aws)\n",
      "18-03-28:11:29:44 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:30:05 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:30:27 INFO     [optimizer.py:337] Upgrading bottleneck node tf-resnet-feats to NodeConfig(tf-resnet-feats, 1, v100, 32.0, 1, aws)\n",
      "18-03-28:11:30:27 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:30:48 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:31:09 INFO     [optimizer.py:337] Upgrading bottleneck node tf-kernel-svm to NodeConfig(tf-kernel-svm, 1, none, 4.0, 1, aws)\n",
      "18-03-28:11:31:09 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:31:30 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:31:51 INFO     [optimizer.py:337] Upgrading bottleneck node inception to NodeConfig(inception, 1, v100, 16.0, 1, aws)\n",
      "18-03-28:11:31:51 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:32:13 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:32:34 INFO     [optimizer.py:337] Upgrading bottleneck node tf-resnet-feats to NodeConfig(tf-resnet-feats, 1, v100, 40.0, 1, aws)\n",
      "18-03-28:11:32:34 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:32:55 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:33:17 INFO     [optimizer.py:337] Upgrading bottleneck node tf-resnet-feats to NodeConfig(tf-resnet-feats, 1, v100, 48.0, 1, aws)\n",
      "18-03-28:11:33:17 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:33:39 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:34:00 INFO     [optimizer.py:337] Upgrading bottleneck node tf-resnet-feats to NodeConfig(tf-resnet-feats, 1, v100, 64.0, 1, aws)\n",
      "18-03-28:11:34:00 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:34:21 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:34:42 INFO     [optimizer.py:337] Upgrading bottleneck node tf-resnet-feats to NodeConfig(tf-resnet-feats, 1, v100, 96.0, 1, aws)\n",
      "18-03-28:11:34:42 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:35:02 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:35:26 INFO     [optimizer.py:337] Upgrading bottleneck node inception to NodeConfig(inception, 1, v100, 24.0, 1, aws)\n",
      "18-03-28:11:35:26 INFO     [optimizer.py:143] Getting max queue\n",
      "18-03-28:11:35:48 INFO     [optimizer.py:329] No more steps can be taken\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'inception': NodeConfig(inception, 1, v100, 24.0, 1, aws),\n",
       "  'tf-kernel-svm': NodeConfig(tf-kernel-svm, 1, none, 4.0, 1, aws),\n",
       "  'tf-log-reg': NodeConfig(tf-log-reg, 1, none, 1, 1, aws),\n",
       "  'tf-resnet-feats': NodeConfig(tf-resnet-feats, 1, v100, 96.0, 1, aws)},\n",
       " {'cost': 5.3219999999999992,\n",
       "  'latency': 0.46440257,\n",
       "  'throughput': 248.0178965950795},\n",
       " 0.47088060739143689)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = GreedyOptimizer(dag, scale_factors, node_profs)\n",
    "cloud = \"aws\"\n",
    "initial_config = {\"inception\": profiler.NodeConfig(name=\"inception\",\n",
    "                                                      num_cpus=1,\n",
    "                                                      gpu_type=\"v100\",\n",
    "                                                      batch_size=1,\n",
    "                                                      num_replicas=1,\n",
    "                                                      cloud=cloud),\n",
    "                  \"tf-resnet-feats\": profiler.NodeConfig(name=\"tf-resnet-feats\",\n",
    "                                                      num_cpus=1,\n",
    "                                                      gpu_type=\"v100\",\n",
    "                                                      batch_size=1,\n",
    "                                                      num_replicas=1,\n",
    "                                                      cloud=cloud),\n",
    "                  \"tf-log-reg\": profiler.NodeConfig(name=\"tf-log-reg\",\n",
    "                                                      num_cpus=1,\n",
    "                                                      gpu_type=\"none\",\n",
    "                                                      batch_size=1,\n",
    "                                                      num_replicas=1,\n",
    "                                                      cloud=cloud),\n",
    "                  \"tf-kernel-svm\": profiler.NodeConfig(name=\"tf-kernel-svm\",\n",
    "                                                      num_cpus=1,\n",
    "                                                      gpu_type=\"none\",\n",
    "                                                      batch_size=1,\n",
    "                                                      num_replicas=1,\n",
    "                                                      cloud=cloud),\n",
    "                 }\n",
    "opt.select_optimal_config(cloud, 0.5, 5.5, initial_config, send_times, optimize_what=\"throughput\", use_netcalc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = GreedyOptimizer(dag, scale_factors, node_profs)\n",
    "cloud = \"aws\"\n",
    "initial_config = {\"inception\": profiler.NodeConfig(name=\"inception\",\n",
    "                                                      num_cpus=1,\n",
    "                                                      gpu_type=\"v100\",\n",
    "                                                      batch_size=1,\n",
    "                                                      num_replicas=1,\n",
    "                                                      cloud=cloud),\n",
    "                  \"tf-resnet-feats\": profiler.NodeConfig(name=\"tf-resnet-feats\",\n",
    "                                                      num_cpus=1,\n",
    "                                                      gpu_type=\"v100\",\n",
    "                                                      batch_size=1,\n",
    "                                                      num_replicas=1,\n",
    "                                                      cloud=cloud),\n",
    "                  \"tf-log-reg\": profiler.NodeConfig(name=\"tf-log-reg\",\n",
    "                                                      num_cpus=1,\n",
    "                                                      gpu_type=\"none\",\n",
    "                                                      batch_size=1,\n",
    "                                                      num_replicas=1,\n",
    "                                                      cloud=cloud),\n",
    "                  \"tf-kernel-svm\": profiler.NodeConfig(name=\"tf-kernel-svm\",\n",
    "                                                      num_cpus=1,\n",
    "                                                      gpu_type=\"none\",\n",
    "                                                      batch_size=1,\n",
    "                                                      num_replicas=1,\n",
    "                                                      cloud=cloud),\n",
    "                 }\n",
    "opt.select_optimal_config(cloud, 0.3, 5.5, initial_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "node_profs[\"res152\"].plot_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in node_profs.items():\n",
    "    p.check_monotonicity()\n",
    "    r = p.plot_profile()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [True, True, False]\n",
    "for i, p in enumerate(b):\n",
    "    print(i,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = node_profs[\"alexnet\"]\n",
    "p.profile.iloc[7][\"mean_batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bundle, _ in r:\n",
    "    print(\"-\".join([str(b) for b in bundle]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt = GreedyOptimizer(dag, scale_factors, node_profs)\n",
    "cloud = \"gcp\"\n",
    "initial_config = {\"tf\": profiler.NodeConfig(name=\"alexnet\",\n",
    "                                                      num_cpus=1,\n",
    "                                                      gpu_type=\"k80\",\n",
    "                                                      batch_size=1,\n",
    "                                                      num_replicas=1,\n",
    "                                                      cloud=cloud),\n",
    "                  \"res50\": profiler.NodeConfig(name=\"res50\",\n",
    "                                                      num_cpus=1,\n",
    "                                                      gpu_type=\"k80\",\n",
    "                                                          batch_size=1,\n",
    "                                                      num_replicas=1,\n",
    "                                                      cloud=cloud),\n",
    "                  \"res152\": profiler.NodeConfig(name=\"res152\",\n",
    "                                                      num_cpus=1,\n",
    "                                                      gpu_type=\"k80\",\n",
    "                                                      batch_size=1,\n",
    "                                                      num_replicas=1,\n",
    "                                                      cloud=cloud),\n",
    "                 }\n",
    "opt.select_optimal_config(cloud, 0.7, 50, initial_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def brute_force_optimizer(dag, scale_factors, node_profs, cost_constraint, latency_constraint):\n",
    "    \"\"\"\n",
    "        This doesn't loo\n",
    "    \"\"\"\n",
    "    all_node_configs = [node_profs[node].enumerate_configs(max_replication_factor=3) for node in dag.nodes()]     \n",
    "    all_pipeline_configs = itertools.product(*all_node_configs)\n",
    "    num_valid_configs = 0\n",
    "    best_config = None\n",
    "    best_config_perf = None\n",
    "    cur_index = 0\n",
    "    for p_config in all_pipeline_configs:\n",
    "        cur_index += 1\n",
    "        if cur_index % 500 == 0:\n",
    "            print(\"Processed {}\".format(cur_index))\n",
    "        cur_node_configs = {n.name: n for n in p_config}\n",
    "        if not profiler.is_valid_pipeline_config(cur_node_configs):\n",
    "            continue\n",
    "        cur_config_perf = profiler.estimate_pipeline_performance_for_config(\n",
    "            dag, scale_factors, cur_node_configs, node_profs)\n",
    "        if cur_config_perf[\"latency\"] <= latency_constraint and cur_config_perf[\"cost\"] <= cost_constraint:\n",
    "            if best_config is None:\n",
    "                best_config = cur_node_configs\n",
    "                best_config_perf = cur_config_perf\n",
    "                print(\"Initializing config to {} ({})\".format(best_config, best_config_perf))\n",
    "            else:\n",
    "                if cur_config_perf[\"throughput\"] > best_config_perf[\"throughput\"]:\n",
    "                    best_config = cur_node_configs\n",
    "                    best_config_perf = cur_config_perf\n",
    "                    print(\"Updating config to {} ({})\".format(best_config, best_config_perf))\n",
    "        \n",
    "    return best_config, best_config_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "brute_force_optimizer(dag, scale_factors, node_profs, 7.0, 0.8)\n",
    "end = datetime.now()\n",
    "print(\"{}\".format((end-start).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profiler.estimate_pipeline_performance_for_config(dag, scale_factors, node_configs, node_profs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = profs[\"alexnet\"].groupby([\"cloud\",\"gpu_type\",\"num_cpus_per_replica\"])\n",
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in groups:\n",
    "    print(name, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
