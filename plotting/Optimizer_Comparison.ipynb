{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardcoded performance behavior of different models. The columns are batchsize, p99 latency in ms, and throughput in qps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_array = np.array([\n",
    "       [1,  132.54010, 10.0],\n",
    "       [2,  134.24957, 19.7],\n",
    "       [4,  138.39000, 38.6],\n",
    "       [8,  149.21285, 76.1],\n",
    "       [12, 152.38963, 105.2],\n",
    "       [16, 172.64560, 126.2],\n",
    "       [24, 212.01610, 168.9],\n",
    "       [32, 256.63817, 191.5]])\n",
    "inception_array=resnet_array[np.argsort(resnet_array[:,0])]\n",
    "inception_array = np.array([\n",
    "       [1,  68.10305,  26.3],\n",
    "       [2,  72.8235,   45.8],\n",
    "       [4,  73.88426,  81.5],\n",
    "       [8,  105.866700, 132.8],\n",
    "       [12, 112.031640, 179.5],\n",
    "       [16, 149.952640, 199.5],\n",
    "       [24, 195.421250, 213.0]])\n",
    "inception_array=inception_array[np.argsort(inception_array[:,0])]\n",
    "ksvm_array = np.array([\n",
    "       [1, 45.305130, 42.3],\n",
    "       [2, 45.864000, 74.300000],\n",
    "       [4, 51.040710, 138.40000],\n",
    "       [8, 54.351710, 276.90000],\n",
    "       [12, 59.511130, 390.00000],\n",
    "       [16, 65.195470, 498.70000],\n",
    "       [24, 75.762700, 655.50000],\n",
    "       [48, 93.624520, 811.50000],\n",
    "       [64, 124.36674, 907.80000]])\n",
    "ksvm_array=ksvm_array[np.argsort(ksvm_array[:,0])]\n",
    "logreg_array = np.array([\n",
    "       [1, 29.28447, 531.5000],\n",
    "       [2, 30.27719, 608.6000],\n",
    "       [4, 31.97647, 1038.900],\n",
    "       [8, 36.15806, 1094.800]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to create a x and y points and create a function that represents the segement connection between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_segment_function(x,y):\n",
    "    def result(x_point):\n",
    "        if x_point in x:\n",
    "            return y[x == x_point][0]\n",
    "        less_index = max(np.arange(len(x))[x < x_point])\n",
    "        more_index = min(np.arange(len(x))[x > x_point])\n",
    "        rise = y[more_index] - y[less_index]\n",
    "        run = x[more_index] - x[less_index]\n",
    "        slope = float(rise)/run\n",
    "        delta = x_point - x[less_index]\n",
    "        return y[less_index] + delta*slope\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A graph representation using nodes as well as a scheduler for simulation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler(object):\n",
    "    # Needed for large values, because np.isclose works on a relative scale. \n",
    "    # It'll think that 300000.4 is close to 300000.2 because of the large size of the numbers relative to their difference\n",
    "    def isclose(self, a, b):\n",
    "        return np.isclose(a-b, 0)\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.time = 0\n",
    "        self.scheduled = [] # A list of items scheduled to run, from earliest to latest\n",
    "    \n",
    "    # Called by the nodes to schedule fn(time_when_fn_executes) to run in_how_long\n",
    "    # Adds the function in its schedule time position relative to other scheduled function\n",
    "    def schedule(self, fn, in_how_long, extra_args = []):\n",
    "        index = 0\n",
    "        current_time_to_schedule = self.time + in_how_long\n",
    "        for item_scheduled_time,_,_ in self.scheduled:\n",
    "             if item_scheduled_time < current_time_to_schedule:\n",
    "                index+=1\n",
    "        self.scheduled.insert(index, [current_time_to_schedule, fn, extra_args])\n",
    "    \n",
    "    # continues to pop scheduled functions from the front of the queue until the queue is empty\n",
    "    # Notice how during execution of the tasks already in the queue more tasks can be scheduled\n",
    "    def start(self):\n",
    "        while self.scheduled != []:\n",
    "            next_event_time, fn, extra_args = self.scheduled.pop(0)\n",
    "            self.time = next_event_time\n",
    "            fn(self.time, *extra_args)\n",
    "            while self.scheduled != [] and self.isclose(self.scheduled[0][0], self.time):\n",
    "                next_event_time, fn, extra_args = self.scheduled.pop(0)\n",
    "                fn(self.time, *extra_args)\n",
    "\n",
    "# Abstract Node class.\n",
    "class Node(object):\n",
    "    def __init__(self, name, scheduler):\n",
    "        self.children = []\n",
    "        self.parents = []\n",
    "        self.name = name\n",
    "        self.scheduler = scheduler\n",
    "    \n",
    "    def then(self, child):\n",
    "        self.children.append(child)\n",
    "        child.parents.append(self)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name+\" Node\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.name+\" Node\"\n",
    "\n",
    "# Initialized by taking a query history and a scheduler\n",
    "# Notice how the queries are actually just dictionary lineages of the node-to-node travel\n",
    "class SourceNode(Node):\n",
    "    def __init__(self, deltas, scheduler):\n",
    "        Node.__init__(self, \"source\", scheduler)\n",
    "        self.deltas = deltas\n",
    "        self.deltas_index = 0\n",
    "        # Schedule the first query to send\n",
    "        self.scheduler.schedule(self.send_query, 0)\n",
    "    \n",
    "    def send_query(self, time):\n",
    "        for child in self.children:\n",
    "            self.scheduler.schedule(child.arrival, 0, extra_args=[{\"id\":self.deltas_index, \"source\":time}])\n",
    "        if self.deltas_index < len(self.deltas):\n",
    "            self.scheduler.schedule(self.send_query, self.deltas[self.deltas_index])\n",
    "            self.deltas_index+=1\n",
    "\n",
    "# A regular model node, with a dynamic, greedy batchsize selection scheme (minimum of maximum batchsize and queue size)\n",
    "# batchsize_p99lat_thru argument is the model-profile array like the one shown in the begining of the notebook\n",
    "# batching_delay argument dictates how long the model waits from the point that it is idle and receives some query that wakes it up to the point that it actually \n",
    "# locks and takes a batchsize from the queue. The reason it was introduced is because longer batching_delay means a more uniform batchsize distribution.\n",
    "class BatchedNode(Node):\n",
    "    def __init__(self, max_batch_size, batchsize_p99lat_thru, name, scheduler, num_replicas=1, batching_delay=0):\n",
    "        Node.__init__(self, name, scheduler)\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.batchsize_p99lat_thru = batchsize_p99lat_thru\n",
    "        self.smoothed_fn = line_segment_function(batchsize_p99lat_thru[:,0], 1/(batchsize_p99lat_thru[:,2]/1000.))\n",
    "        self.scheduler = scheduler\n",
    "        self.batching_delay = batching_delay\n",
    "        self.queue = []\n",
    "        self.queue_size_over_time = []\n",
    "        self.batch_sizes_over_time = []\n",
    "        self.num_replicas = num_replicas\n",
    "        self.in_model = [[] for _ in range(self.num_replicas)]\n",
    "        self.model_idle = [True for _ in range(self.num_replicas)]\n",
    "        self.num_arrivals = 1\n",
    "    \n",
    "    def arrival(self, time, query):\n",
    "        self.num_arrivals+=1\n",
    "        query[self.name+\"_enqueue_time\"] = time\n",
    "        self.queue.append(query)\n",
    "        self.queue_size_over_time.append([time, len(self.queue)])\n",
    "        import math\n",
    "        # the number of models that might need waking up, equals to the number of maximum batches in the queue, rounding up\n",
    "        number_to_wake_up = math.ceil(len(self.queue) / float(self.max_batch_size))\n",
    "        for model_index in range(len(self.model_idle)):\n",
    "            if self.model_idle[model_index]:\n",
    "                self.scheduler.schedule(self.model_take, self.batching_delay, extra_args=[model_index])\n",
    "                self.model_idle[model_index] = False\n",
    "                number_to_wake_up-=1\n",
    "            if number_to_wake_up == 0:\n",
    "                break\n",
    "            \n",
    "    def model_take(self, time, model_index):\n",
    "        assert len(self.queue) > 0\n",
    "        num_taken_to_process = min(len(self.queue), self.max_batch_size)\n",
    "        self.in_model[model_index] = self.queue[:num_taken_to_process]\n",
    "        self.queue = self.queue[num_taken_to_process:] # dequeue from queue\n",
    "        for query in self.in_model[model_index]:\n",
    "            query[self.name+\"_dequeue_time\"] = time\n",
    "            query[self.name+\"_batchsize\"] = num_taken_to_process\n",
    "            query_id = query[\"id\"]\n",
    "        self.queue_size_over_time.append([time, len(self.queue)])\n",
    "        self.batch_sizes_over_time.append([time, num_taken_to_process])\n",
    "        self.scheduler.schedule(self.model_return, self.smoothed_fn(num_taken_to_process), extra_args=[model_index]) # service time of batch\n",
    "    \n",
    "    def model_return(self, time, model_index):\n",
    "        for query in self.in_model[model_index]:\n",
    "            query[self.name+\"_service_time\"] = time\n",
    "            for child in self.children:\n",
    "                self.scheduler.schedule(child.arrival, 0, extra_args=[query])\n",
    "        self.in_model[model_index] = []\n",
    "        if len(self.queue) == 0:\n",
    "            self.model_idle[model_index] = True\n",
    "        else:\n",
    "            self.scheduler.schedule(self.model_take, self.batching_delay, extra_args=[model_index])\n",
    "\n",
    "# Waits for queries of the same id to arrive from all the parents before sending the union of the lineages to the children\n",
    "class JoinNode(Node):\n",
    "    def __init__(self, scheduler):\n",
    "        Node.__init__(self, \"join\", scheduler)\n",
    "        self.waiting_values = {} # maps from id to list of queries\n",
    "    \n",
    "    def union_dicts(self, dict_list):\n",
    "        if len(dict_list) == 0:\n",
    "            return {}\n",
    "        result = {}\n",
    "        for dict_element in dict_list:\n",
    "            result = dict(result, **dict_element)\n",
    "        return result\n",
    "    \n",
    "    def arrival(self, time, query):\n",
    "        query_id = query[\"id\"]\n",
    "        if self.waiting_values.get(query_id) == None:\n",
    "            self.waiting_values[query_id] = [query]\n",
    "        else:\n",
    "            self.waiting_values[query_id].append(query)\n",
    "        if len(self.waiting_values[query_id]) == len(self.parents):\n",
    "            query_union = self.union_dicts(self.waiting_values[query_id])\n",
    "            for child in self.children:\n",
    "                self.scheduler.schedule(child.arrival, 0, extra_args=[query_union]) \n",
    "        \n",
    "# Prints every 1000 queries that end up in it, the entire lineage at the end is found at self.queue \n",
    "class SinkNode(Node):\n",
    "    def __init__(self, scheduler):\n",
    "        Node.__init__(self, \"sink\", scheduler)\n",
    "        self.queue = []\n",
    "        self.counter = 0\n",
    "    \n",
    "    def arrival(self, time, query):\n",
    "        self.counter+=1\n",
    "        if self.counter % 1000 == 0:\n",
    "            print self.counter\n",
    "        query[\"sink\"] = time\n",
    "        self.queue.append(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Image Driver 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of image driver 1 graph\n",
    "# The hardware configuration for each node\n",
    "# The model behavior for each node\n",
    "# The arrival history\n",
    "\n",
    "model_configs = {\n",
    "    \"Inception\": {\"bs\": 1, \"rf\": 1, \"hw\": \"V100\"},\n",
    "    \"ResNet\":    {\"bs\": 1, \"rf\": 1, \"hw\": \"V100\"},\n",
    "    \"KSVM\":      {\"bs\": 1, \"rf\": 1, \"hw\": \"none\"},\n",
    "    \"LogReg\":    {\"bs\": 1, \"rf\": 1, \"hw\": \"none\"}\n",
    "}\n",
    "\n",
    "\n",
    "# Gives batchsize, p99, throughput array based on hardware placement for each model\n",
    "# Here it assumes just V100s for GPU models\n",
    "def model_behaviors_from_configs(model_configs):\n",
    "    model_behaviors = {\n",
    "        \"Inception\":inception_array,\n",
    "        \"ResNet\":resnet_array,\n",
    "        \"KSVM\":ksvm_array,\n",
    "        \"LogReg\":logreg_array\n",
    "    }\n",
    "    return model_behaviors\n",
    "\n",
    "# Returns maximum end-to-end throughput of pipeline (assuming maximum batchsize)\n",
    "def end_to_end_max_throughput(model_configs, model_behaviors):\n",
    "    resnet_bs = model_configs[\"ResNet\"][\"bs\"]\n",
    "    inception_bs = model_configs[\"Inception\"][\"bs\"]\n",
    "    ksvm_bs = model_configs[\"KSVM\"][\"bs\"]\n",
    "    logreg_bs = model_configs[\"LogReg\"][\"bs\"]\n",
    "    resnet_branch = min(resnet_throughput, ksvm_throughput)\n",
    "    inception_branch = min(inception_througput, ksvm_throughput)\n",
    "    return min(resnet_branch, inception_branch)\n",
    "\n",
    "# Returns minimum end-to-end latency of pipeline (not accounting for queuing effects)\n",
    "def end_to_end_min_lat(model_configs, model_behaviors):\n",
    "    pass\n",
    "    \n",
    "def get_end_to_end_times(model_configs, model_behaviors, deltas):\n",
    "    scheduler = Scheduler()\n",
    "    source = SourceNode(deltas, scheduler)\n",
    "    inception = BatchedNode(max_batch_size=model_configs[\"Inception\"][\"bs\"], \n",
    "                            batchsize_p99lat_thru=model_behaviors[\"Inception\"], \n",
    "                            name=\"Inception\", scheduler=scheduler, batching_delay=0, \n",
    "                            num_replicas=model_configs[\"Inception\"][\"rf\"]) \n",
    "    resnet    = BatchedNode(max_batch_size=model_configs[\"ResNet\"][\"bs\"], \n",
    "                            batchsize_p99lat_thru=model_behaviors[\"ResNet\"], \n",
    "                            name=\"ResNet\", scheduler=scheduler, batching_delay=0, \n",
    "                            num_replicas=model_configs[\"ResNet\"][\"rf\"])\n",
    "    ksvm      = BatchedNode(max_batch_size=model_configs[\"KSVM\"][\"bs\"], \n",
    "                            batchsize_p99lat_thru=model_behaviors[\"KSVM\"], \n",
    "                            name=\"KSVM\", scheduler=scheduler, batching_delay=0, \n",
    "                            num_replicas=model_configs[\"KSVM\"][\"rf\"])\n",
    "    logreg    = BatchedNode(max_batch_size=model_configs[\"LogReg\"][\"bs\"], \n",
    "                            batchsize_p99lat_thru=model_behaviors[\"LogReg\"], \n",
    "                            name=\"LogReg\", scheduler=scheduler, batching_delay=0, \n",
    "                            num_replicas=model_configs[\"LogReg\"][\"rf\"])\n",
    "    join = JoinNode(scheduler)\n",
    "    sink = SinkNode(scheduler)\n",
    "    source.then(resnet)\n",
    "    source.then(inception)\n",
    "    inception.then(logreg)\n",
    "    resnet.then(ksvm)\n",
    "    logreg.then(join)\n",
    "    ksvm.then(join)\n",
    "    join.then(sink)\n",
    "    scheduler.start()\n",
    "    final_lineage_result = sink.queue\n",
    "    return final_lineage_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "lambd = 20\n",
    "CV = 0.1\n",
    "with open(\"../experiments/cached_arrival_processes/{}_{}.deltas\".format(lambd, CV), 'r') as f:\n",
    "    deltas = np.array([float(l.strip()) for l in f]).flatten()\n",
    "\n",
    "queue = get_end_to_end_times(model_configs, model_behaviors_from_configs(model_configs), deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Inception_batchsize': 1,\n",
       " 'Inception_dequeue_time': 0,\n",
       " 'Inception_enqueue_time': 0,\n",
       " 'Inception_service_time': 38.02281368821293,\n",
       " 'KSVM_batchsize': 1,\n",
       " 'KSVM_dequeue_time': 100.0,\n",
       " 'KSVM_enqueue_time': 100.0,\n",
       " 'KSVM_service_time': 123.64066193853428,\n",
       " 'LogReg_batchsize': 1,\n",
       " 'LogReg_dequeue_time': 38.02281368821293,\n",
       " 'LogReg_enqueue_time': 38.02281368821293,\n",
       " 'LogReg_service_time': 39.90428123289778,\n",
       " 'ResNet_batchsize': 1,\n",
       " 'ResNet_dequeue_time': 0,\n",
       " 'ResNet_enqueue_time': 0,\n",
       " 'ResNet_service_time': 100.0,\n",
       " 'id': 0,\n",
       " 'sink': 123.64066193853428,\n",
       " 'source': 0}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queue[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    print \"hello {}\".format(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello 2\n"
     ]
    }
   ],
   "source": [
    "f(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
