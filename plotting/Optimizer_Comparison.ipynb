{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardcoded performance behavior of different models. The columns are batchsize, p99 latency in ms, and throughput in qps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_v100 = np.array([\n",
    "    [1,  132.54010, 10.0],\n",
    "    [2,  134.24957, 19.7],\n",
    "    [4,  138.39000, 38.6],\n",
    "    [8,  149.21285, 76.1],\n",
    "    [12, 152.38963, 105.2],\n",
    "    [16, 172.64560, 126.2],\n",
    "    [24, 212.01610, 168.9],\n",
    "    [32, 256.63817, 191.5]])\n",
    "resnet_v100=resnet_v100[np.argsort(resnet_v100[:,0])]\n",
    "resnet_TPX = np.array([\n",
    "    [1,  108.87625, 9.9],\n",
    "    [2,  111.04000, 20.4],\n",
    "    [4,  121.55254, 39.9],\n",
    "    [8,  141.82200, 73.6],\n",
    "    [16, 152.72976, 126.7],\n",
    "    [32, 278.38175, 151.6]])\n",
    "resnet_TPX=resnet_TPX[np.argsort(resnet_TPX[:,0])]\n",
    "inception_v100 = np.array([\n",
    "    [1,  68.10305,  26.3],\n",
    "    [2,  72.8235,   45.8],\n",
    "    [4,  73.88426,  81.5],\n",
    "    [8,  105.866700, 132.8],\n",
    "    [12, 112.031640, 179.5],\n",
    "    [16, 149.952640, 199.5],\n",
    "    [24, 195.421250, 213.0]])\n",
    "inception_v100=inception_v100[np.argsort(inception_v100[:,0])]\n",
    "inception_TPX = np.array([\n",
    "    [1, 34.84264, 32.4],\n",
    "    [2, 38.78975, 61.9],\n",
    "    [4, 57.46735, 103.8],\n",
    "    [8, 74.90487, 137.7],\n",
    "    [16, 123.04392, 174.8],\n",
    "    [32, 216.74803, 203.0]])\n",
    "inception_TPX=inception_TPX[np.argsort(inception_TPX[:,0])]\n",
    "ksvm_cpu = np.array([\n",
    "       [1, 45.305130, 42.3],\n",
    "       [2, 45.864000, 74.300000],\n",
    "       [4, 51.040710, 138.40000],\n",
    "       [8, 54.351710, 276.90000],\n",
    "       [12, 59.511130, 390.00000],\n",
    "       [16, 65.195470, 498.70000],\n",
    "       [24, 75.762700, 655.50000],\n",
    "       [48, 93.624520, 811.50000],\n",
    "       [64, 124.36674, 907.80000]])\n",
    "ksvm_cpu=ksvm_cpu[np.argsort(ksvm_cpu[:,0])]\n",
    "logreg_cpu = np.array([\n",
    "       [1, 29.28447, 531.5000],\n",
    "       [2, 30.27719, 608.6000],\n",
    "       [4, 31.97647, 1038.900],\n",
    "       [8, 36.15806, 1094.800]])\n",
    "logreg_cpu=logreg_cpu[np.argsort(logreg_cpu[:,0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary from hardware to the model's behavior on that hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hardware_behavior = {\n",
    "    \"ResNet\":    {\"TPX\":resnet_TPX, \"V100\":resnet_v100},\n",
    "    \"Inception\": {\"TPX\":inception_TPX, \"V100\":inception_v100},\n",
    "    \"KSVM\":      {\"cpu\":ksvm_cpu},\n",
    "    \"logreg\":    {\"cpu\":logreg_cpu}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Known hardware list and moving around between hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list must be sorted by cost\n",
    "hardware = [\n",
    "    (\"V100\", 3),\n",
    "    (\"TXP\", 2),\n",
    "    (\"cpu\", 1)\n",
    "]\n",
    "\n",
    "def upgrade_hardware(model, current_hardware):\n",
    "    # Find index of current_hardware in hardware list\n",
    "    index = hardware.index(current_hardware)\n",
    "    while index != 0:\n",
    "        if hardware[index-1] in model_hardware_behavior[model]:\n",
    "            return hardware[index-1]\n",
    "        index-=1\n",
    "    return None\n",
    "\n",
    "def downgrade_hardware(model, current_hardware):\n",
    "    # Find index of current_hardware in hardware list\n",
    "    index = hardware.index(current_hardware)\n",
    "    while index != len(hardware)-1:\n",
    "        if hardware[index+1] in model_hardware_behavior[model]:\n",
    "            return hardware[index+1]\n",
    "        index+=1\n",
    "    return None\n",
    "\n",
    "def increase_batchsize(model, current_hardware, current_batchsize):\n",
    "    model_behavior = model_hardware_behavior[model][current_hardware]\n",
    "    batchsizes = model_behavior[:,0]\n",
    "    index = batchsizes.index(current_batchsize)\n",
    "    if index == len(batchsizes)-1:\n",
    "        return None\n",
    "    else:\n",
    "        return batchsizes[index+1]\n",
    "\n",
    "def decrease_batchsize(model, current_hardware, current_batchsize):\n",
    "    model_behavior = model_hardware_behavior[model][current_hardware]\n",
    "    batchsizes = model_behavior[:,0]\n",
    "    index = batchsizes.index(current_batchsize)\n",
    "    if index == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return batchsizes[index-1]\n",
    "    \n",
    "model_configs = {\n",
    "    \"Inception\": {\"bs\":16, \"rf\": 1, \"hw\": \"V100\"},\n",
    "    \"ResNet\":    {\"bs\":16, \"rf\": 1, \"hw\": \"V100\"},\n",
    "    \"KSVM\":      {\"bs\":16, \"rf\": 1, \"hw\": \"none\"},\n",
    "    \"LogReg\":    {\"bs\":2, \"rf\": 1, \"hw\": \"none\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A function to create a x and y points and create a function that represents the segement connection between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_segment_function(x,y):\n",
    "    def result(x_point):\n",
    "        assert x_point <= max(x) and x_point >= min(x), \"{} out of range of {}\".format(x_point, x)\n",
    "        if x_point in x:\n",
    "            return y[x == x_point][0]\n",
    "        less_index = max(np.arange(len(x))[x < x_point])\n",
    "        more_index = min(np.arange(len(x))[x > x_point])\n",
    "        rise = y[more_index] - y[less_index]\n",
    "        run = x[more_index] - x[less_index]\n",
    "        slope = float(rise)/run\n",
    "        delta = x_point - x[less_index]\n",
    "        return y[less_index] + delta*slope\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A graph representation using nodes as well as a scheduler for simulation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler(object):\n",
    "    # Needed for large values, because np.isclose works on a relative scale. \n",
    "    # It'll think that 300000.4 is close to 300000.2 because of the large size of the numbers relative to their difference\n",
    "    def isclose(self, a, b):\n",
    "        return np.isclose(a-b, 0)\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.time = 0\n",
    "        self.scheduled = [] # A list of items scheduled to run, from earliest to latest\n",
    "    \n",
    "    # Called by the nodes to schedule fn(time_when_fn_executes) to run in_how_long\n",
    "    # Adds the function in its schedule time position relative to other scheduled function\n",
    "    def schedule(self, fn, in_how_long, extra_args = []):\n",
    "        index = 0\n",
    "        current_time_to_schedule = self.time + in_how_long\n",
    "        for item_scheduled_time,_,_ in self.scheduled:\n",
    "             if item_scheduled_time < current_time_to_schedule:\n",
    "                index+=1\n",
    "        self.scheduled.insert(index, [current_time_to_schedule, fn, extra_args])\n",
    "    \n",
    "    # continues to pop scheduled functions from the front of the queue until the queue is empty\n",
    "    # Notice how during execution of the tasks already in the queue more tasks can be scheduled\n",
    "    def start(self):\n",
    "        while self.scheduled != []:\n",
    "            next_event_time, fn, extra_args = self.scheduled.pop(0)\n",
    "            self.time = next_event_time\n",
    "            fn(self.time, *extra_args)\n",
    "            while self.scheduled != [] and self.isclose(self.scheduled[0][0], self.time):\n",
    "                next_event_time, fn, extra_args = self.scheduled.pop(0)\n",
    "                fn(self.time, *extra_args)\n",
    "\n",
    "# Abstract Node class.\n",
    "class Node(object):\n",
    "    def __init__(self, name, scheduler):\n",
    "        self.children = []\n",
    "        self.parents = []\n",
    "        self.name = name\n",
    "        self.scheduler = scheduler\n",
    "    \n",
    "    def then(self, child):\n",
    "        self.children.append(child)\n",
    "        child.parents.append(self)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.name+\" Node\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.name+\" Node\"\n",
    "\n",
    "# Initialized by taking a query history and a scheduler\n",
    "# Notice how the queries are actually just dictionary lineages of the node-to-node travel\n",
    "class SourceNode(Node):\n",
    "    def __init__(self, deltas, scheduler):\n",
    "        Node.__init__(self, \"source\", scheduler)\n",
    "        self.deltas = deltas\n",
    "        self.deltas_index = 0\n",
    "        # Schedule the first query to send\n",
    "        self.scheduler.schedule(self.send_query, 0)\n",
    "    \n",
    "    def send_query(self, time):\n",
    "        for child in self.children:\n",
    "            self.scheduler.schedule(child.arrival, 0, extra_args=[[{\"id\":self.deltas_index, \"source\":time}]])\n",
    "        if self.deltas_index < len(self.deltas):\n",
    "            self.scheduler.schedule(self.send_query, self.deltas[self.deltas_index])\n",
    "            self.deltas_index+=1\n",
    "\n",
    "# A regular model node, with a dynamic, greedy batchsize selection scheme (minimum of maximum batchsize and queue size)\n",
    "# batchsize_p99lat_thru argument is the model-profile array like the one shown in the begining of the notebook\n",
    "# batching_delay argument dictates how long the model waits from the point that it is idle and receives some query that wakes it up to the point that it actually \n",
    "# locks and takes a batchsize from the queue. The reason it was introduced is because longer batching_delay means a more uniform batchsize distribution.\n",
    "class BatchedNode(Node):\n",
    "    def __init__(self, max_batch_size, batchsize_p99lat_thru, name, scheduler, num_replicas=1, batching_delay=0):\n",
    "        Node.__init__(self, name, scheduler)\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.batchsize_p99lat_thru = batchsize_p99lat_thru\n",
    "        self.smoothed_fn = line_segment_function(batchsize_p99lat_thru[:,0], 1/(batchsize_p99lat_thru[:,2]/1000.))\n",
    "        self.scheduler = scheduler\n",
    "        self.batching_delay = batching_delay\n",
    "        self.queue = []\n",
    "        self.queue_size_over_time = []\n",
    "        self.batch_sizes_over_time = []\n",
    "        self.num_replicas = num_replicas\n",
    "        self.in_model = [[] for _ in range(self.num_replicas)]\n",
    "        self.model_idle = [True for _ in range(self.num_replicas)]\n",
    "        self.num_arrivals = 1\n",
    "    \n",
    "    def arrival(self, time, query_list):\n",
    "        self.num_arrivals+=1\n",
    "        for query in query_list:\n",
    "            query[self.name+\"_enqueue_time\"] = time\n",
    "            self.queue_size_over_time.append([time, len(self.queue)])\n",
    "        self.queue.extend(query_list)\n",
    "        import math\n",
    "        # the number of models that might need waking up, equals to the number of maximum batches in the queue, rounding up\n",
    "        number_to_wake_up = math.ceil(len(self.queue) / float(self.max_batch_size))\n",
    "        for model_index in range(len(self.model_idle)):\n",
    "            if self.model_idle[model_index]:\n",
    "                self.scheduler.schedule(self.model_take, self.batching_delay, extra_args=[model_index])\n",
    "                self.model_idle[model_index] = False\n",
    "                number_to_wake_up-=1\n",
    "            if number_to_wake_up == 0:\n",
    "                break\n",
    "            \n",
    "    def model_take(self, time, model_index):\n",
    "        assert len(self.queue) > 0\n",
    "        num_taken_to_process = min(len(self.queue), self.max_batch_size)\n",
    "        self.in_model[model_index] = self.queue[:num_taken_to_process]\n",
    "        self.queue = self.queue[num_taken_to_process:] # dequeue from queue\n",
    "        for query in self.in_model[model_index]:\n",
    "            query[self.name+\"_dequeue_time\"] = time\n",
    "            query[self.name+\"_batchsize\"] = num_taken_to_process\n",
    "            query_id = query[\"id\"]\n",
    "        self.queue_size_over_time.append([time, len(self.queue)])\n",
    "        self.batch_sizes_over_time.append([time, num_taken_to_process])\n",
    "        self.scheduler.schedule(self.model_return, self.smoothed_fn(num_taken_to_process), extra_args=[model_index]) # service time of batch\n",
    "    \n",
    "    def model_return(self, time, model_index):\n",
    "        for query in self.in_model[model_index]:\n",
    "            query[self.name+\"_service_time\"] = time\n",
    "        query_batch_copy = list(self.in_model[model_index])\n",
    "        for child in self.children:\n",
    "            self.scheduler.schedule(child.arrival, 0, extra_args=[query_batch_copy])\n",
    "        self.in_model[model_index] = []\n",
    "        if len(self.queue) == 0:\n",
    "            self.model_idle[model_index] = True\n",
    "        else:\n",
    "            self.scheduler.schedule(self.model_take, self.batching_delay, extra_args=[model_index])\n",
    "\n",
    "# Waits for queries of the same id to arrive from all the parents before sending the union of the lineages to the children\n",
    "class JoinNode(Node):\n",
    "    def __init__(self, scheduler):\n",
    "        Node.__init__(self, \"join\", scheduler)\n",
    "        self.waiting_values = {} # maps from id to list of queries\n",
    "    \n",
    "    def union_dicts(self, dict_list):\n",
    "        if len(dict_list) == 0:\n",
    "            return {}\n",
    "        result = {}\n",
    "        for dict_element in dict_list:\n",
    "            result = dict(result, **dict_element)\n",
    "        return result\n",
    "    \n",
    "    def arrival(self, time, query_list):\n",
    "        query_output_list = []\n",
    "        for query in query_list:\n",
    "            query_id = query[\"id\"]\n",
    "            if self.waiting_values.get(query_id) == None:\n",
    "                self.waiting_values[query_id] = [query]\n",
    "            else:\n",
    "                self.waiting_values[query_id].append(query)\n",
    "            if len(self.waiting_values[query_id]) == len(self.parents):\n",
    "                query_union = self.union_dicts(self.waiting_values[query_id])\n",
    "                query_output_list.append(query_union)\n",
    "        if query_output_list != []:\n",
    "            for child in self.children:\n",
    "                self.scheduler.schedule(child.arrival, 0, extra_args=[query_output_list]) \n",
    "        \n",
    "# Prints every 1000 queries that end up in it, the entire lineage at the end is found at self.queue \n",
    "class SinkNode(Node):\n",
    "    def __init__(self, scheduler):\n",
    "        Node.__init__(self, \"sink\", scheduler)\n",
    "        self.queue = []\n",
    "        self.counter = 0\n",
    "    \n",
    "    def arrival(self, time, query_list):\n",
    "        for query in query_list:\n",
    "            self.counter+=1\n",
    "            if self.counter % 1000 == 0:\n",
    "                print self.counter\n",
    "            query[\"sink\"] = time\n",
    "        self.queue.extend(query_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Image Driver 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-195-794e5882e633>, line 67)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-195-794e5882e633>\"\u001b[0;36m, line \u001b[0;32m67\u001b[0m\n\u001b[0;31m    if check_feasibility()\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "example_image_driver_1_config = {\n",
    "    \"ResNet\":    {\"bs\":1, \"rf\": 1, \"hw\":\"V100\"},\n",
    "    \"Inception\": {\"bs\":1, \"rf\": 1, \"hw\":\"V100\"},\n",
    "    \"KSVM\":      {\"bs\":1, \"rf\": 1, \"hw\":\"cpu\"},\n",
    "    \"LogReg\":    {\"bs\":1, \"rf\": 1, \"hw\":\"cpu\"}\n",
    "}\n",
    "\n",
    "def model_behavior_from_config(model, config):\n",
    "    return model_hardware_behavior[model][model_configs[model][\"hw\"]]\n",
    "\n",
    "def get_throughput(model, hardware, batchsize):\n",
    "    model_behavior = model_hardware_behavior[model][hardware]\n",
    "    return model_behavior[model_behavior[:,0].index(batchsize), 3]\n",
    "\n",
    "# Returns maximum end-to-end throughput of pipeline (assuming maximum batchsize)\n",
    "def end_to_end_max_throughput(model_configs):\n",
    "    def get_throughput_for_batchsize(model):\n",
    "        bs = model_configs[model][\"bs\"]\n",
    "        model_behavior = model_behavior_from_config(model, model_configs)\n",
    "        return model_behavior[model_behavior[:,0] == bs][3]\n",
    "    resnet_throughput = get_throughput_for_batchsize(\"ResNet\")\n",
    "    inception_througput = get_throughput_for_batchsize(\"Inception\")\n",
    "    ksvm_throughput = get_throughput_for_batchsize(\"KSVM\")\n",
    "    logreg_latency = get_throughput_for_batchsize(\"LogReg\")\n",
    "    resnet_branch = min(resnet_throughput, ksvm_throughput)\n",
    "    inception_branch = min(inception_througput, logreg_latency)\n",
    "    return min(resnet_branch, inception_branch)\n",
    "\n",
    "def end_to_end_minimum_latency(model_configs):\n",
    "    def get_latency_for_batchsize(model):\n",
    "        bs = model_configs[model][\"bs\"]\n",
    "        model_behavior = model_behavior_from_config(model, model_configs)\n",
    "        return model_behavior[model_behavior[:, 0] == bs][2]\n",
    "    resnet_latency = get_latency_for_batchsize(\"ResNet\")\n",
    "    inceptionlatency = get_latency_for_batchsize(\"Inception\")\n",
    "    ksvm_latency = get_latency_for_batchsize(\"KSVM\")\n",
    "    logreg_latency = get_latency_for_batchsize(\"LogReg\")\n",
    "    resnet_branch = resnet_latency + ksvm_latency\n",
    "    inception_branch = inception_latency + logreg_latency\n",
    "    return max(resnet_branch, inception_branch)\n",
    "    \n",
    "def get_end_to_end_times(model_configs, deltas):\n",
    "    scheduler = Scheduler()\n",
    "    source = SourceNode(deltas, scheduler)\n",
    "    def create_batched_node(model):\n",
    "        return BatchedNode(max_batch_size=model_configs[model][\"bs\"], \n",
    "                           batchsize_p99lat_thru=model_behavior_from_config(model, model_configs), \n",
    "                           name=model, scheduler=scheduler, batching_delay=0, \n",
    "                           num_replicas=model_configs[model][\"rf\"]) \n",
    "    inception = create_batched_node(\"Inception\")\n",
    "    resnet    = create_batched_node(\"ResNet\")\n",
    "    ksvm      = create_batched_node(\"KSVM\")\n",
    "    logreg    = create_batched_node(\"LogReg\")\n",
    "    join = JoinNode(scheduler)\n",
    "    sink = SinkNode(scheduler)\n",
    "    source.then(resnet)\n",
    "    source.then(inception)\n",
    "    inception.then(logreg)\n",
    "    resnet.then(ksvm)\n",
    "    logreg.then(join)\n",
    "    ksvm.then(join)\n",
    "    join.then(sink)\n",
    "    scheduler.start()\n",
    "    final_lineage_result = sink.queue\n",
    "    return final_lineage_result\n",
    "\n",
    "def check_feasibility(model_configs, deltas, slo):\n",
    "    mean_arrival_throughput = 1/np.mean(deltas)*1000 # average arrival throughput in qps\n",
    "    max_pipeline_throughput = end_to_end_max_throughput(model_configs) # max throughput in qps\n",
    "    if mean_arrival_throughput > max_pipeline_throughput:\n",
    "        return False\n",
    "    min_pipeline_latency = end_to_end_minimum_latency(model_configs) # min latency in ms\n",
    "    if min_pipeline_latency > slo:\n",
    "        return False\n",
    "    end_to_end_times = get_end_to_end_times(model_configs, deltas)\n",
    "    if np.max(end_to_end_times) > slo:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def throughput_maximizer(cost, slo, deltas):\n",
    "    # Start with bs=1, rf=1, hw=smallest for all models\n",
    "    initial_config = None\n",
    "    # in each iteration, find bottleneck model\n",
    "    # try to upgrade by first increasing batchsize, then increasing rf, then upgrading hardware\n",
    "    # if none of these steps work, then you're done\n",
    "    # otherwise continue iterating\n",
    "\n",
    "def cost_minimizer(slo, deltas):\n",
    "    # Start configuration with bs=1, rf=N, hw=largest for all models, where N is the minimum number of rfs needed to make the pipeline feasible\n",
    "    initial_config = {\n",
    "        \"ResNet\":    {\"bs\":1, \"rf\": 1, \"hw\":\"V100\"},\n",
    "        \"Inception\": {\"bs\":1, \"rf\": 1, \"hw\":\"V100\"},\n",
    "        \"KSVM\":      {\"bs\":1, \"rf\": 1, \"hw\":\"cpu\"},\n",
    "        \"LogReg\":    {\"bs\":1, \"rf\": 1, \"hw\":\"cpu\"}\n",
    "    }\n",
    "    arrival_lambda = 1/np.mean(deltas)*1000.\n",
    "    import math\n",
    "    for m in initial_config:\n",
    "        throughput = model_behavior_from_config(m, initial_config)\n",
    "        replicas = math.ceil(arrival_lambda / throughput)\n",
    "        config[m][\"rf\"] = replicas\n",
    "    print initial_config\n",
    "    something_changed = True\n",
    "    while something_changed:\n",
    "        something_changed = False\n",
    "        # in each iteration, iterate through all of the models\n",
    "        for m in initial_config:\n",
    "            next_batchsize = increase_batchsize(m, initial_config[m][\"hw\"], initial_config[m][\"bs\"])\n",
    "            if next_batchsize != None:\n",
    "                initial_config[m][\"bs\"] = next_batchsize\n",
    "                something_changed = True\n",
    "            \n",
    "    # for each model, see if you can increase the batchsize, then decrease the rf, then to downgrade hardware\n",
    "    # iteration ends when none of the models could be changed\n",
    "\n",
    "\n",
    "def bruteforce_optimizer(cost, slo, deltas):\n",
    "    # Get the maximum rf needed for each model (when its batchsize is 1 and hw is the lowest) and set that as the bound of rf range to look at\n",
    "    # lowest_cost_seen = np.inf\n",
    "    # Pick a configuration at random\n",
    "    # If its cost is less than lowest_cost_seen then check feasibility, otherwise skip it and prune all configurations more expensive than it\n",
    "    # If its feasible then update lowest_cost_seen, and prune all configurations \n",
    "    # Start at a random configuration\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "lambd = 100\n",
    "CV = 4.0\n",
    "with open(\"../experiments/cached_arrival_processes/{}_{}.deltas\".format(lambd, CV), 'r') as f:\n",
    "    deltas = np.array([float(l.strip()) for l in f]).flatten()\n",
    "\n",
    "queue = get_end_to_end_times(model_configs, model_behaviors_from_configs(model_configs), deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
